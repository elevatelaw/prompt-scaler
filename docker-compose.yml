# Docker Compose file for running a LiteLLM proxy.

name: prompt_scaler_proxy

services:
  litellm:
    # See https://github.com/BerriAI/litellm/issues/9888 for an issue introduced in 1.65.4.
    image: ghcr.io/berriai/litellm:main-v1.65.0-stable
    command:
      - --config
      - /app/config.yaml
      - --detailed_debug
    container_name: litellm
    ports:
      - "4000:4000"
    # This does fall over every once in a while, sometimes because of memory leaks
    # under heavy load?
    restart: on-failure
    deploy:
      resources:
        limits:
          # Set some kind of appropriate memory limit so that LiteLLM
          # can't cause the host to OOM. You may want to adjust this
          # for your server and your level of parallelism.
          memory: 20G
    environment:
      # OpenAI
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      # Anthropic
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      # Gemini
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      # Ollama
      - OLLAMA_API_BASE=http://172.17.0.1:11434
    volumes:
      - ./litellm_config.yml:/app/config.yaml
    #restart: unless-stopped