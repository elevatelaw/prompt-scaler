# LiteLLM configuration as test proxy.
litellm_settings:
  # See what we're actually sending and receiving.
  # Very helpful for debugging.
  log_raw_request_response: True
model_list:
  - model_name: gpt-4o-mini
    litellm_params:
      model: openai/gpt-4o-mini
      api_key: os.environ/OPENAI_API_KEY
  - model_name: claude-3-5-haiku-20241022
    litellm_params:
      model: anthropic/claude-3-5-haiku-20241022
      api_key: os.environ/ANTHROPIC_API_KEY
      # drop_params does not seem to work here?
      drop_params: True
      additional_drop_params: ["store"]
  - model_name: gemini-2.0-flash
    litellm_params:
      model: gemini/gemini-2.0-flash
      api_key: os.environ/GEMINI_API_KEY
  - model_name: gemini-2.5-flash-preview-04-17-thinking
    litellm_params:
      model: gemini/gemini-2.5-flash-preview-04-17
      api_key: os.environ/GEMINI_API_KEY
      # Sadly, these parameters are not passed through, no matter what the
      # documentation says.
      #
      # generationConfig:
      #   thinkingConfig:
      #     # Thinking is expensive!
      #     thinkingBudget: 0
  - model_name: gemma3:12b
    litellm_params:
      model: ollama/gemma3:12b
      api_base: http://172.17.0.1:11434
      ssl_verify: false
  - model_name: llama3.2-vision
    litellm_params:
      model: ollama/llama3.2-vision
      api_base: http://172.17.0.1:11434
      ssl_verify: false
#general_settings:
#  master_key: sk-1234
