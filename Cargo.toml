[package]
name = "prompt-scaler"
version = "0.2.12"
edition = "2024"
authors = ["eric.kidd@elevate.law"]

description = "Run parameterized LLM prompts at scale, including OCR."
categories = ["command-line-utilities", "template-engine"]
keywords = ["llm", "ocr"]
license = "MIT OR Apache-2.0"
readme = "README.md"
homepage = "https://github.com/elevatelaw/prompt-scaler"
repository = "https://github.com/elevatelaw/prompt-scaler"

[dependencies]
anyhow = "1.0.97"
assert_cmd = "2.0.16"
async-openai = { version = "0.28.1", default-features = false, features = [
    "byot",
    "rustls",
] }
async-trait = "0.1.88"
aws-config = "1.6.1"
aws-sdk-textract = "1.64.0"
base64 = "0.22.1"
clap = { version = "4.5.28", features = ["derive", "wrap_help"] }
codespan-reporting = "0.12.0"
csv = "1.3.1"
csv-async = { version = "1.3.0", features = ["tokio"] }
dotenvy = "0.15.7"
futures = "0.3.31"
genai = "0.2.3"
handlebars = "6.3.2"
handlebars-concat = "0.3.0"
infer = "0.19.0"
indicatif = { version = "0.17.11", features = ["futures"] }
jsonschema = { version = "0.30.0", default-features = false }
keen-retry = "0.5.0"
leaky-bucket = "1.1.2"
mime_guess = "2.0.5"
num_cpus = "1.16.0"
peekable = { version = "0.3.0", features = ["tokio"] }
regex = "1.11.1"
reqwest = { version = "0.12.15", default-features = false }
schemars = "0.8.22"
serde = { version = "1.0.219", features = ["derive"] }
serde_json = "1.0.140"
tempfile = "3.19.1"
tokio = { version = "1.44.1", features = [
    "macros",
    "tracing",
    "rt-multi-thread",
    "fs",
    "io-std",
    "io-util",
    "process",
    "sync",
    "time",
    "tracing",
] }
tokio-stream = { version = "0.1.17", features = ["io-util"] }
toml-span = { version = "0.5.2", features = ["reporting"] }
tracing = "0.1.41"
tracing-subscriber = { version = "0.3.19", features = ["env-filter"] }
