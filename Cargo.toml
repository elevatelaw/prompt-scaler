[package]
name = "prompt-scaler"
version = "0.1.0"
edition = "2024"
authors = ["eric.kidd@elevate.law"]

description = "Run parameterized LLM prompts at scale, including OCR."
categories = ["command-line-utilities", "template-engine"]
keywords = ["llm", "ocr"]
license = "MIT OR Apache-2.0"
readme = "README.md"
homepage = "https://github.com/elevatelaw/prompt-scaler"
repository = "https://github.com/elevatelaw/prompt-scaler"

[dependencies]
anyhow = "1.0.97"
assert_cmd = "2.0.16"
async-openai = { version = "0.28.0", features = ["byot"] }
async-trait = "0.1.88"
aws-config = "1.6.1"
aws-sdk-textract = "1.64.0"
base64 = "0.22.1"
clap = { version = "4.5.28", features = ["derive", "wrap_help"] }
csv = "1.3.1"
csv-async = { version = "1.3.0", features = ["tokio"] }
dotenvy = "0.15.7"
futures = "0.3.31"
handlebars = "6.3.2"
infer = "0.19.0"
#indicatif = "0.17.11"
jsonschema = { version = "0.29.1", features = ["resolve-async"] }
keen-retry = "0.5.0"
leaky-bucket = "1.1.2"
mime_guess = "2.0.5"
peekable = { version = "0.3.0", features = ["tokio"] }
reqwest = "0.12.15"
schemars = "0.8.22"
serde = { version = "1.0.219", features = ["derive"] }
serde_json = "1.0.140"
tempfile = "3.19.1"
tokio = { version = "1.44.1", features = [
    "macros",
    "tracing",
    "rt-multi-thread",
    "fs",
    "io-std",
    "io-util",
    "process",
    "sync",
    "time",
    "tracing",
] }
tokio-stream = { version = "0.1.17", features = ["io-util"] }
toml = "0.8.20"
tracing = "0.1.41"
#tracing-indicatif = "0.3.9"
tracing-subscriber = { version = "0.3.19", features = ["env-filter"] }
